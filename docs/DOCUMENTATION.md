# Documentation Exhaustive - Moteur d'√âvaluation Formelle par Expression Affine

**Auteur:** Guillaume BERTHELOT  
**Version:** 1.0  
**Date:** 2025

---

## Table des mati√®res

1. [Introduction](#1-introduction)
2. [Fondements Th√©oriques](#2-fondements-th√©oriques)
3. [Architecture Logicielle](#3-architecture-logicielle)
4. [Guide d'Installation](#4-guide-dinstallation)
5. [Guide d'Utilisation](#5-guide-dutilisation)
6. [API de R√©f√©rence](#6-api-de-r√©f√©rence)
7. [Exemples Pratiques](#7-exemples-pratiques)
8. [Tests et Validation](#8-tests-et-validation)
9. [Optimisation et Performance](#9-optimisation-et-performance)
10. [Limitations et Extensions](#10-limitations-et-extensions)
11. [FAQ](#11-faq)
12. [R√©f√©rences](#12-r√©f√©rences)

---

## 1. Introduction

### 1.1 Contexte

Les r√©seaux de neurones profonds sont vuln√©rables aux **attaques adversariales** : de petites perturbations imperceptibles de l'entr√©e peuvent conduire √† des classifications erron√©es. Cette vuln√©rabilit√© pose des probl√®mes critiques dans les applications sensibles (voitures autonomes, diagnostic m√©dical, s√©curit√©).

### 1.2 Probl√©matique

Les approches empiriques (tests Monte Carlo, attaques adversariales) ne garantissent pas l'exhaustivit√© :
- ‚ùå Impossibilit√© de tester tous les cas
- ‚ùå Pas de garantie math√©matique
- ‚ùå Co√ªt computationnel √©lev√©

### 1.3 Solution Propos√©e

Ce projet impl√©mente une **v√©rification formelle** par propagation d'expressions affines qui :
- ‚úÖ Garantit math√©matiquement les bornes de sortie
- ‚úÖ Couvre tous les cas possibles d'entr√©es bruit√©es
- ‚úÖ Identifie les classes certifi√©es robustes
- ‚úÖ Fournit des m√©triques quantitatives de robustesse

### 1.4 Avantages de l'Approche

| Crit√®re | Approche Empirique | Approche Formelle (ce projet) |
|---------|-------------------|-------------------------------|
| Garantie | ‚ùå Non | ‚úÖ Oui (math√©matique) |
| Exhaustivit√© | ‚ùå Partielle | ‚úÖ Compl√®te |
| Temps | ‚ö†Ô∏è Variable | ‚úÖ D√©terministe |
| Pr√©cision | ‚ö†Ô∏è Stochastique | ‚úÖ Bornes certifi√©es |

---

## 2. Fondements Th√©oriques

### 2.1 Expressions Affines

#### 2.1.1 D√©finition

Une expression affine repr√©sente une combinaison lin√©aire de variables symboliques :

```
y = a‚ÇÄ + Œ£·µ¢ (a·µ¢ ¬∑ x·µ¢)    avec x·µ¢ ‚àà [l·µ¢, u·µ¢]
```

O√π :
- `a‚ÇÄ` : constante (terme ind√©pendant)
- `a·µ¢` : coefficient de la variable `x·µ¢`
- `[l·µ¢, u·µ¢]` : bornes de la variable `x·µ¢`

#### 2.1.2 Calcul des Bornes

Pour une expression affine `y`, les bornes `[min(y), max(y)]` se calculent par :

```
min(y) = a‚ÇÄ + Œ£·µ¢ (a·µ¢ ‚â• 0 ? a·µ¢¬∑l·µ¢ : a·µ¢¬∑u·µ¢)
max(y) = a‚ÇÄ + Œ£·µ¢ (a·µ¢ ‚â• 0 ? a·µ¢¬∑u·µ¢ : a·µ¢¬∑l·µ¢)
```

**Exemple :**
```
y = 2 + 3x‚ÇÅ - x‚ÇÇ  avec x‚ÇÅ ‚àà [0, 1], x‚ÇÇ ‚àà [1, 2]

min(y) = 2 + 3¬∑0 - 1¬∑2 = 0
max(y) = 2 + 3¬∑1 - 1¬∑1 = 4
```

### 2.2 Propagation √† Travers les Couches

#### 2.2.1 Couches Lin√©aires

Pour une couche lin√©aire `y = Wx + b` :

```python
# Pour chaque neurone de sortie y·µ¢
y·µ¢ = b·µ¢ + Œ£‚±º (w·µ¢‚±º ¬∑ x‚±º)

# Si x‚±º est une expression affine : x‚±º = a‚ÇÄ‚ÅΩ ≤‚Åæ + Œ£‚Çñ (a‚Çñ‚ÅΩ ≤‚Åæ¬∑Œµ‚Çñ)
# Alors y·µ¢ devient :
y·µ¢ = b·µ¢ + Œ£‚±º w·µ¢‚±º¬∑(a‚ÇÄ‚ÅΩ ≤‚Åæ + Œ£‚Çñ a‚Çñ‚ÅΩ ≤‚Åæ¬∑Œµ‚Çñ)
   = (b·µ¢ + Œ£‚±º w·µ¢‚±º¬∑a‚ÇÄ‚ÅΩ ≤‚Åæ) + Œ£‚Çñ (Œ£‚±º w·µ¢‚±º¬∑a‚Çñ‚ÅΩ ≤‚Åæ)¬∑Œµ‚Çñ
```

**Propri√©t√© :** Les op√©rations lin√©aires pr√©servent la forme affine.

#### 2.2.2 Couches Convolutionnelles

Pour une convolution 2D :

```
y‚Çí·µ§‚Çú[b,c_out,h,w] = Œ£c_in Œ£kh Œ£kw (w[c_out,c_in,kh,kw] ¬∑ x[b,c_in,h',w']) + bias[c_out]
```

O√π `h' = h¬∑stride + kh - padding` et `w' = w¬∑stride + kw - padding`

**Impl√©mentation :** Chaque pixel de sortie est une combinaison lin√©aire des pixels d'entr√©e ‚Üí pr√©serve la forme affine.

### 2.3 Relaxation des Activations Non-Lin√©aires

#### 2.3.1 ReLU : Cas d'√âtude Principal

Pour `z = ReLU(y) = max(0, y)` avec `y ‚àà [l, u]` :

**Cas 1 : Toujours inactif** (`u ‚â§ 0`)
```
z = 0
```

**Cas 2 : Toujours actif** (`l ‚â• 0`)
```
z = y  (pr√©serve l'expression affine)
```

**Cas 3 : Ambigu√Øt√©** (`l < 0 < u`)
```
Relaxation lin√©aire (sur-approximation conservative) :
z ‚àà [0, u]
z ‚â§ (u/(u-l))¬∑(y - l)

Expression affine r√©sultante :
z = 0 + (u/(u-l))¬∑[coefficients de y]
avec nouvelles bornes √©largies
```

**Illustration graphique :**

```
    z |
    u |     /----  (borne sup√©rieure relax√©e)
      |    /
      |   /
    0 |__/________
      l  0     u   y
```

#### 2.3.2 Autres Activations

**Sigmoid :** `œÉ(y) = 1/(1+e‚Åª ∏)`
```
Approximation lin√©aire au point milieu m = (l+u)/2 :
œÉ(y) ‚âà œÉ(m) + œÉ'(m)¬∑(y - m)
avec œÉ'(m) = œÉ(m)¬∑(1 - œÉ(m))
```

**Tanh :** `tanh(y)`
```
Approximation lin√©aire similaire :
tanh(y) ‚âà tanh(m) + (1 - tanh¬≤(m))¬∑(y - m)
```

### 2.4 MaxPooling : Relaxation Conservative

Pour `z = max(y‚ÇÅ, y‚ÇÇ, ..., y‚Çô)` :

```
Bornes conservatives :
min(z) = max(min(y‚ÇÅ), min(y‚ÇÇ), ..., min(y‚Çô))
max(z) = max(max(y‚ÇÅ), max(y‚ÇÇ), ..., max(y‚Çô))

Expression affine :
On prend l'expression avec max(y·µ¢) le plus √©lev√©
et on √©largit les bornes pour couvrir tous les cas
```

### 2.5 Soundness (Correction)

**D√©finition :** Les bornes calcul√©es sont **sound** si et seulement si :

```
‚àÄx ‚àà r√©gion bruit√©e, f(x) ‚àà [min_formel, max_formel]
```

**V√©rification statistique :**
1. √âchantillonner N entr√©es bruit√©es al√©atoirement
2. Calculer f(x) pour chaque √©chantillon
3. V√©rifier que tous les f(x) ‚àà bornes formelles
4. Si violation ‚Üí impl√©mentation incorrecte

---

## 3. Architecture Logicielle

### 3.1 Vue d'Ensemble

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Interface Utilisateur                  ‚îÇ
‚îÇ  (CLI: affine_eval.py, API Python, Scripts de test)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 Moteur Principal                         ‚îÇ
‚îÇ  evaluate_model() - Orchestration du workflow           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ              ‚îÇ              ‚îÇ                 ‚îÇ
     ‚ñº              ‚ñº              ‚ñº                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ONNX   ‚îÇ  ‚îÇ Affine   ‚îÇ  ‚îÇ   Bound     ‚îÇ  ‚îÇ  Result    ‚îÇ
‚îÇ Parser  ‚îÇ  ‚îÇ Engine   ‚îÇ  ‚îÇ Propagator  ‚îÇ  ‚îÇ Aggregator ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                   ‚îÇ
                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                         ‚îÇ   NonLinear        ‚îÇ
                         ‚îÇ   Relaxer          ‚îÇ
                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.2 Modules Principaux

#### 3.2.1 `affine_engine.py`

**R√¥le :** Repr√©sentation et manipulation des expressions affines

**Classes :**

```python
class AffineExpression:
    """
    Repr√©sente : y = a‚ÇÄ + Œ£·µ¢(a·µ¢¬∑x·µ¢)
    
    Attributs:
        constant: float - a‚ÇÄ
        coefficients: Dict[int, float] - {i: a·µ¢}
        bounds: Dict[int, Tuple[float, float]] - {i: (l·µ¢, u·µ¢)}
    
    M√©thodes:
        get_bounds() -> (float, float)
        __add__, __mul__ : Op√©rations arithm√©tiques
    """

class AffineExpressionEngine:
    """
    Moteur de transformation des expressions
    
    M√©thodes statiques:
        create_input_expressions(data, noise) -> List[AffineExpression]
        linear_layer(exprs, W, b) -> List[AffineExpression]
        conv2d_layer(...) -> Tuple[List[AffineExpression], shape]
        maxpool2d_layer(...) -> Tuple[List[AffineExpression], shape]
        avgpool2d_layer(...) -> Tuple[List[AffineExpression], shape]
    """
```

**Complexit√© :**
- `create_input_expressions`: O(n) o√π n = nombre de pixels
- `linear_layer`: O(m¬∑k¬∑s) o√π m=sorties, k=entr√©es, s=symboles moyens
- `conv2d_layer`: O(C_out¬∑H_out¬∑W_out¬∑C_in¬∑K_h¬∑K_w¬∑s)

#### 3.2.2 `onnx_parser.py`

**R√¥le :** Extraction de l'architecture et des poids depuis ONNX

**Classe :**

```python
class ONNXParser:
    """
    Parse un mod√®le ONNX
    
    M√©thodes:
        parse() -> List[Dict[str, Any]]
            Retourne la liste des couches avec leurs param√®tres
        
        get_input_shape() -> tuple
        get_output_names() -> List[str]
    """
```

**Format de sortie :**

```python
{
    'type': 'Conv',  # Type d'op√©ration ONNX
    'name': 'conv1',
    'inputs': ['input'],
    'outputs': ['conv1_out'],
    'attributes': {
        'strides': [1, 1],
        'pads': [1, 1, 1, 1],
        'dilations': [1, 1]
    },
    'weights': np.ndarray,  # (C_out, C_in, K_h, K_w)
    'bias': np.ndarray      # (C_out,)
}
```

#### 3.2.3 `relaxer.py`

**R√¥le :** Impl√©mentation des relaxations pour activations non-lin√©aires

**Classe :**

```python
class NonLinearRelaxer:
    """
    Relaxations des fonctions non-lin√©aires
    
    M√©thodes statiques:
        relu_relaxation(expr, type='linear') -> AffineExpression
        sigmoid_relaxation(expr) -> AffineExpression
        tanh_relaxation(expr) -> AffineExpression
    """
```

**Algorithme ReLU Relaxation :**

```python
def relu_relaxation(expr, relaxation_type='linear'):
    l, u = expr.get_bounds()
    
    if u <= 0:
        return zero_expression()
    elif l >= 0:
        return expr  # Identit√©
    else:
        # Relaxation lin√©aire
        slope = u / (u - l)
        return scale_expression(expr, slope, offset=-l*slope)
```

---

## 4. Guide d'Installation

## 5. Guide d'Utilisation

## 6. API de R√©f√©rence

## 7. Exemples Pratiques

## 8. Tests et Validation

## 9. Optimisation et Performance

## 10. Limitations et Extensions

## 11. FAQ

### 11.1 Questions G√©n√©rales

**Q : Quelle est la diff√©rence entre √©valuation formelle et tests empiriques ?**

**R :** 
- **Tests empiriques** (Monte Carlo) : √âchantillonnent al√©atoirement des cas et v√©rifient la robustesse sur ces √©chantillons. Pas de garantie sur les cas non test√©s.
- **√âvaluation formelle** : Calcule math√©matiquement les bornes exactes couvrant **tous** les cas possibles. Garantie exhaustive.

**Q : Pourquoi mes bornes sont-elles si larges ?**

**R :** Plusieurs raisons possibles :
1. **Niveau de bruit √©lev√©** : Plus Œµ est grand, plus les bornes s'√©largissent
2. **Relaxations conservatives** : Les relaxations ReLU/MaxPool sur-approximent
3. **Propagation de l'incertitude** : L'incertitude se cumule √† travers les couches
4. **Complexit√© du r√©seau** : R√©seaux profonds ‚Üí bornes plus larges

**Solution :** Utiliser un niveau de bruit plus faible ou optimiser le r√©seau pour la robustesse.

**Q : Mon mod√®le n'est pas support√©, que faire ?**

**R :** V√©rifiez que :
1. Le mod√®le est export√© en ONNX opset 11+
2. Toutes les couches utilis√©es sont support√©es (voir section 3.2)
3. Pas de couches dynamiques (Dropout en mode training, etc.)

Pour ajouter une nouvelle couche, cr√©ez une m√©thode dans `affine_engine.py` et `bound_propagator.py`.

### 11.2 Questions Techniques

**Q : Comment interpr√©ter les r√©sultats de soundness ?**

**R :** Le rapport de soundness v√©rifie que les bornes formelles sont correctes :
- **is_sound = True** : ‚úÖ Impl√©mentation correcte, toutes les valeurs observ√©es sont dans les bornes
- **is_sound = False** : ‚ùå Bug dans l'impl√©mentation, certaines valeurs d√©passent les bornes
- **violation_rate** : % de violations (doit √™tre 0%)
- **marges de s√©curit√©** : Distance entre valeurs observ√©es et bornes (plus c'est large, mieux c'est)

**Q : Que signifie "classe robuste certifi√©e" ?**

**R :** Une classe est **robuste certifi√©e** si sa borne inf√©rieure est strictement sup√©rieure aux bornes sup√©rieures de toutes les autres classes :

```
classe_robuste si: min(score(classe)) > max(score(autres_classes))
```

Cela garantit que **toute** perturbation dans la boule de bruit donnera cette classe.

**Q : Pourquoi l'√©valuation formelle est-elle plus lente que l'inf√©rence standard ?**

**R :** L'√©valuation formelle :
- Manipule des expressions symboliques (pas juste des nombres)
- Maintient des coefficients pour chaque variable de bruit
- Calcule des bornes √† chaque √©tape

**Complexit√© :**
- Inf√©rence standard : O(n) o√π n = nombre d'op√©rations
- √âvaluation formelle : O(n ¬∑ s¬≤) o√π s = nombre moyen de symboles

**Q : Comment r√©duire le temps d'ex√©cution ?**

**R :** Plusieurs strat√©gies :
1. **R√©duire le niveau de bruit** : Moins de complexit√© symbolique
2. **Simplifier les expressions** : Supprimer les coefficients n√©gligeables
3. **Utiliser des approximations plus grossi√®res** : Trade-off pr√©cision/vitesse
4. **Activer le cache** : R√©utiliser les calculs de bornes
5. **Future : GPU** : Parall√©lisation massive

### 11.3 Questions de D√©veloppement

**Q : Comment ajouter une nouvelle couche (ex: LayerNorm) ?**

**R :** Suivez ces √©tapes :

1. **Ajouter la m√©thode dans `affine_engine.py` :**
```python
@staticmethod
def layernorm_layer(expressions: List[AffineExpression],
                   normalized_shape: tuple,
                   weight: np.ndarray,
                   bias: np.ndarray,
                   eps: float = 1e-5) -> List[AffineExpression]:
    """
    Impl√©mente LayerNorm
    Note: LayerNorm est non-lin√©aire, n√©cessite une relaxation
    """
    # Votre impl√©mentation ici
    pass
```

2. **Ajouter la propagation dans `bound_propagator.py` :**
```python
elif layer_type == 'LayerNorm':
    expressions = self._propagate_layernorm(expressions, layer)
```

3. **Ajouter la m√©thode priv√©e :**
```python
def _propagate_layernorm(self, expressions, layer):
    attrs = layer.get('attributes', {})
    # Extraire les param√®tres et appeler l'engine
    return self.engine.layernorm_layer(expressions, ...)
```

4. **Ajouter des tests dans `tests/` :**
```python
def test_layernorm_propagation(self):
    # Tester la propagation LayerNorm
    pass
```

**Q : Comment d√©bugger une violation de soundness ?**

**R :** Proc√©dure syst√©matique :

1. **Identifier la couche probl√©matique** :
```python
# Activer le mode verbeux
propagator = BoundPropagator(enable_reporting=True)
# Examiner intermediate_bounds pour chaque couche
```

2. **V√©rifier les bornes couche par couche** :
```python
for layer_bounds in results['intermediate_bounds']:
    print(f"Couche {layer_bounds['layer']}: {layer_bounds['bounds']}")
```

3. **Tester une couche isol√©ment** :
```python
# Cr√©er des expressions de test
test_expr = AffineExpression(...)
result = engine.linear_layer([test_expr], weights, bias)
# V√©rifier manuellement les bornes
```

4. **Comparer avec une impl√©mentation de r√©f√©rence** :
```python
# Ex: ERAN, AI2, ou impl√©mentation manuelle
```

**Q : Comment contribuer au projet ?**

**R :** Bienvenue ! Voici comment :

1. **Fork le d√©p√¥t**
2. **Cr√©er une branche** : `git checkout -b feature/ma-feature`
3. **D√©velopper avec tests** : Ajouter des tests unitaires
4. **V√©rifier la qualit√©** :
```bash
python -m unittest discover tests/
black modules/ tests/  # Formatter le code
flake8 modules/ tests/  # V√©rifier le style
```
5. **Commit et Push** :
```bash
git commit -m "feat: Ajout de LayerNorm"
git push origin feature/ma-feature
```
6. **Cr√©er une Pull Request**

**Conventions :**
- Messages de commit : `feat:`, `fix:`, `docs:`, `test:`
- Docstrings : Format Google Style
- Tests : Coverage > 80%

### 11.4 Questions de D√©bogage

**Q : Erreur "ValueError: input_shape doit √™tre sp√©cifi√©" ?**

**R :** Le propagateur a perdu la trace de la forme des tenseurs. Solutions :
1. V√©rifier que `current_shape` est correctement mis √† jour √† chaque couche
2. Passer explicitement `input_shape` √† `propagate()`
3. V√©rifier que les op√©rations de reshape sont correctement impl√©ment√©es

**Q : Erreur "IndexError: index out of bounds" dans Conv2D ?**

**R :** Probl√®me de padding ou de stride. V√©rifications :
```python
# Calculer les dimensions de sortie
out_h = (in_h + 2*pad_h - dil_h*(kernel_h-1) - 1) // stride_h + 1
out_w = (in_w + 2*pad_w - dil_w*(kernel_w-1) - 1) // stride_w + 1

# V√©rifier que out_h, out_w > 0
assert out_h > 0 and out_w > 0
```

**Q : Les bornes sont n√©gatives alors qu'elles devraient √™tre positives ?**

**R :** V√©rifier :
1. **Apr√®s ReLU** : Les bornes doivent √™tre ‚â• 0
2. **Relaxation correcte** : `relu_relaxation()` g√®re les 3 cas
3. **Propagation des bornes** : `get_bounds()` calcule correctement min/max

Debug :
```python
# Avant ReLU
l, u = expr_before_relu.get_bounds()
print(f"Avant ReLU: [{l}, {u}]")

# Apr√®s ReLU
expr_after = relaxer.relu_relaxation(expr_before_relu)
l2, u2 = expr_after.get_bounds()
print(f"Apr√®s ReLU: [{l2}, {u2}]")
assert l2 >= 0, "Borne inf√©rieure n√©gative apr√®s ReLU!"
```

---

## 12. R√©f√©rences

### 12.1 Publications Scientifiques

**V√©rification Formelle de R√©seaux de Neurones :**

1. **DeepPoly** (POPL 2019)
   - *Singh, G., Gehr, T., P√ºschel, M., & Vechev, M.*
   - "An Abstract Domain for Certifying Neural Networks"
   - https://dl.acm.org/doi/10.1145/3290354

2. **AI2** (NeurIPS 2018)
   - *Gehr, T., Mirman, M., Drachsler-Cohen, D., et al.*
   - "AI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation"
   - https://proceedings.neurips.cc/paper/2018/hash/2ecd2bd94734e5dd392d8a896ac6bb18-Abstract.html

3. **CROWN** (NeurIPS 2018)
   - *Zhang, H., Weng, T. W., Chen, P. Y., Hsieh, C. J., & Daniel, L.*
   - "Efficient Neural Network Robustness Certification with General Activation Functions"
   - https://arxiv.org/abs/1811.00866

4. **Interval Bound Propagation** (ICLR 2019)
   - *Gowal, S., Dvijotham, K., Stanforth, R., et al.*
   - "Scalable Verified Training for Provably Robust Image Classification"
   - https://arxiv.org/abs/1811.12774

**Robustesse Adversariale :**

5. **Certified Defenses** (ICML 2018)
   - *Cohen, J., Rosenfeld, E., & Kolter, Z.*
   - "Certified Adversarial Robustness via Randomized Smoothing"
   - https://arxiv.org/abs/1902.02918

6. **Fast-Lin** (ICML 2018)
   - *Weng, T. W., Zhang, H., Chen, H., et al.*
   - "Towards Fast Computation of Certified Robustness for ReLU Networks"
   - https://arxiv.org/abs/1804.09699

7. **MILP Verification** (CAV 2017)
   - *Tjeng, V., Xiao, K., & Tedrake, R.*
   - "Evaluating Robustness of Neural Networks with Mixed Integer Programming"
   - https://arxiv.org/abs/1711.07356

**Analyse Abstraite :**

8. **Abstract Interpretation** (1977)
   - *Cousot, P., & Cousot, R.*
   - "Abstract Interpretation: A Unified Lattice Model for Static Analysis"
   - Fondements th√©oriques de l'analyse abstraite

### 12.2 Outils et Frameworks

**Outils de V√©rification Existants :**

- **ERAN** (ETH Zurich)
  - https://github.com/eth-sri/eran
  - Supporte DeepPoly, DeepZono, RefineZono
  
- **auto_LiRPA** (Robust ML Lab)
  - https://github.com/Verified-Intelligence/auto_LiRPA
  - Automatic Linear Relaxation based Perturbation Analysis

- **CROWN** (IBM Research)
  - https://github.com/IBM/CROWN-Robustness-Certification
  - Certification avec fonctions d'activation g√©n√©rales

- **Œ±,Œ≤-CROWN** (Competition Winner)
  - https://github.com/Verified-Intelligence/alpha-beta-CROWN
  - Gagnant VNN-COMP 2021, 2022, 2023

- **Marabou** (Stanford)
  - https://github.com/NeuralNetworkVerification/Marabou
  - SMT-based neural network verification

**Biblioth√®ques Python :**

- **ONNX** : https://github.com/onnx/onnx
- **ONNXRuntime** : https://github.com/microsoft/onnxruntime
- **PyTorch** : https://pytorch.org/
- **NumPy** : https://numpy.org/

### 12.3 Datasets

- **MNIST** : http://yann.lecun.com/exdb/mnist/
- **Fashion-MNIST** : https://github.com/zalandoresearch/fashion-mnist
- **CIFAR-10/100** : https://www.cs.toronto.edu/~kriz/cifar.html
- **ImageNet** : https://www.image-net.org/

### 12.4 Benchmarks de V√©rification

- **VNN-COMP** (International Verification of Neural Networks Competition)
  - https://sites.google.com/view/vnn2023
  - Benchmarks standardis√©s, comp√©tition annuelle

- **ACAS Xu** (Airborne Collision Avoidance System)
  - Benchmark classique pour v√©rification de r√©seaux de neurones
  - https://github.com/guykatzz/ReluplexCav2017

### 12.5 Cours et Tutoriels

**Cours en Ligne :**

1. **"Reliable and Interpretable Artificial Intelligence"** (ETH Zurich)
   - https://www.sri.inf.ethz.ch/teaching/riai2023
   - Cours complet sur la v√©rification formelle

2. **"Deep Learning Security"** (UC Berkeley)
   - https://dl-security.github.io/
   - Focus sur robustesse adversariale

3. **"Formal Methods for ML"** (CMU)
   - Fondamentaux de v√©rification formelle

**Tutoriels :**

- **PyTorch ‚Üí ONNX Export** : https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html
- **Abstract Interpretation Tutorial** : https://www.di.ens.fr/~cousot/AI/IntroAbsInt.html

### 12.6 Articles de Blog et Ressources

1. **"A Practical Guide to Neural Network Verification"**
   - Blog post d√©taill√© sur les m√©thodes de v√©rification

2. **"Understanding Adversarial Examples"**
   - https://adversarial-ml-tutorial.org/

3. **"Certified Robustness Explained"**
   - Introduction accessible aux concepts de certification

### 12.7 Communaut√© et Support

**Forums et Discussions :**

- **r/MachineLearning** (Reddit)
- **ML Security Slack**
- **ONNX Community** : https://onnx.ai/community.html

**Conf√©rences Pertinentes :**

- **NeurIPS** : Neural Information Processing Systems
- **ICML** : International Conference on Machine Learning
- **ICLR** : International Conference on Learning Representations
- **CAV** : Computer Aided Verification
- **S&P** : IEEE Symposium on Security and Privacy

### 12.8 Citations Sugg√©r√©es

Si vous utilisez ce projet dans vos recherches, veuillez citer :

```bibtex
@misc{berthelot2024affine,
  title={Moteur d'√âvaluation Formelle par Expression Affine},
  author={Berthelot, Guillaume},
  year={2024},
  howpublished={\url{https://github.com/username/AbstClaud}},
  note={Outil de v√©rification formelle pour r√©seaux de neurones}
}
```

---

## Annexes

### Annexe A : Glossaire

**Termes Techniques :**

- **Expression Affine** : Combinaison lin√©aire de variables symboliques avec bornes
- **Relaxation** : Sur-approximation d'une fonction non-lin√©aire par une fonction plus simple
- **Soundness** : Propri√©t√© garantissant que les bornes calcul√©es contiennent toutes les valeurs possibles
- **Completeness** : Propri√©t√© garantissant que les bornes sont les plus serr√©es possibles (ce projet ne garantit pas la completeness)
- **Perturbation L‚àû** : Bruit born√© pixel par pixel : |x' - x|‚àû ‚â§ Œµ
- **Perturbation L2** : Bruit born√© en norme euclidienne : ||x' - x||‚ÇÇ ‚â§ Œµ
- **Certification** : Preuve math√©matique de robustesse
- **Attaque Adversariale** : Perturbation optimis√©e pour tromper le r√©seau

**Acronymes :**

- **ONNX** : Open Neural Network Exchange
- **CNN** : Convolutional Neural Network
- **ReLU** : Rectified Linear Unit
- **FC** : Fully Connected (layer)
- **API** : Application Programming Interface
- **CLI** : Command Line Interface
- **GPU** : Graphics Processing Unit

### Annexe B : Format des R√©sultats JSON

Structure compl√®te du fichier `results.json` :

```json
{
  "bounds_per_class": {
    "class_0": [lower, upper],
    "class_1": [lower, upper],
    ...
  },
  "robust_class": "class_5" | null,
  "intermediate_bounds": [
    {
      "layer": "conv1",
      "type": "Conv",
      "bounds": [[l1, u1], [l2, u2], ...],
      "shape": [1, 16, 28, 28]
    },
    ...
  ],
  "execution_time": 3.14,
  "noise_level": 0.05,
  "activation_relaxation": "linear",
  "detailed_report": {
    "total_layers": 12,
    "total_execution_time": 3.14,
    "total_expressions_processed": 15680,
    "total_symbols": 2352000,
    "avg_symbols_per_layer": 196000,
    "layer_types": {
      "Conv": {"count": 2, "total_time": 2.5, ...},
      ...
    },
    "timestamp": "2024-01-15T10:30:00"
  }
}
```

### Annexe C : Table de Complexit√©

| Op√©ration | Entr√©e | Sortie | Complexit√© Temporelle | Complexit√© Spatiale |
|-----------|--------|--------|----------------------|---------------------|
| Linear | (n, s) | (m, s) | O(n ¬∑ m ¬∑ s) | O(m ¬∑ s) |
| Conv2D | (C_in¬∑H¬∑W, s) | (C_out¬∑H'¬∑W', s) | O(C_in¬∑C_out¬∑K¬≤¬∑H'¬∑W'¬∑s) | O(C_out¬∑H'¬∑W'¬∑s) |
| MaxPool | (C¬∑H¬∑W, s) | (C¬∑H'¬∑W', s) | O(C¬∑H'¬∑W'¬∑K¬≤¬∑s) | O(C¬∑H'¬∑W'¬∑s) |
| ReLU | (n, s) | (n, s) | O(n ¬∑ s) | O(n ¬∑ s) |

O√π : s = nombre moyen de symboles par expression

### Annexe D : Checklist de D√©ploiement

**Avant d√©ploiement en production :**

- [ ] Tests unitaires passent (100%)
- [ ] Tests de soundness OK (violation_rate = 0%)
- [ ] Performance acceptable (< 60s pour mod√®les cibles)
- [ ] Documentation √† jour
- [ ] Logging configur√©
- [ ] Gestion d'erreurs robuste
- [ ] Validation des entr√©es utilisateur
- [ ] Limites de ressources (timeout, m√©moire)
- [ ] Monitoring mis en place
- [ ] Backups configur√©s

### Annexe E : Historique des Versions

**Version 1.0.0** (2024-01-15)
- ‚ú® Premi√®re version stable
- ‚ú® Support Conv2D, MaxPool, Linear, ReLU
- ‚ú® Soundness checker int√©gr√©
- ‚ú® Rapports d√©taill√©s
- ‚ú® Tests complets

**Version 0.9.0** (2024-01-01)
- üîß Beta release
- üîß Impl√©mentation des couches de base

**Version 0.5.0** (2023-12-15)
- üöß Alpha release
- üöß Proof of concept

---

## Contact et Support

**Auteur :** Guillaume BERTHELOT

**Issues :** https://github.com/username/AbstClaud/issues

**Discussions :** https://github.com/username/AbstClaud/discussions

**Email :** [Votre email]

---

**Derni√®re mise √† jour :** 2024-01-15

**Version de la documentation :** 1.0

---

*Cette documentation est maintenue activement. Pour toute question, correction ou suggestion, n'h√©sitez pas √† ouvrir une issue sur GitHub.*
